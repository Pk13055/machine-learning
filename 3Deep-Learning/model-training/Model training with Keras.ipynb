{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling a test network using Keras\n",
    "\n",
    "## Introduction\n",
    "\n",
    "I will be using keras with dense layers and a Sequential model. Herein, we will be using standard data from [this](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data) archive for _diabetic data_. First, let us import the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data and parsing\n",
    "Now that we have import all the required files, let us first import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "filename = 'dataset.csv'\n",
    "assert(filename) in os.listdir()\n",
    "\n",
    "raw_data = open(filename).read().strip(' ').strip('\\n').split('\\n')\n",
    "dataset = [[float(x) for x in y.split(',')] for y in raw_data ]\n",
    "dataset = np.array(dataset)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the dataset, we can move to making the X and y features and then assembling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('X = ',\n",
       " array([[   6.   ,  148.   ,   72.   , ...,   33.6  ,    0.627,   50.   ],\n",
       "        [   1.   ,   85.   ,   66.   , ...,   26.6  ,    0.351,   31.   ],\n",
       "        [   8.   ,  183.   ,   64.   , ...,   23.3  ,    0.672,   32.   ],\n",
       "        ..., \n",
       "        [   5.   ,  121.   ,   72.   , ...,   26.2  ,    0.245,   30.   ],\n",
       "        [   1.   ,  126.   ,   60.   , ...,   30.1  ,    0.349,   47.   ],\n",
       "        [   1.   ,   93.   ,   70.   , ...,   30.4  ,    0.315,   23.   ]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset[:, :-1]\n",
    "y = dataset[:, -1]\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "'X = ', X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling the model\n",
    "\n",
    "After the data has been imported, we can start assembling the model. According to our input features, we will model the network as a _3-layer_, network with the **reLu** activation function. Our first layer has **8 input features** (matching the shape of the X vector); it has a _relu_ activation function and consists of _12 nodes_. The second layer has _12 nodes_ and a _relu_ activation function. Being a binary classification problem, we can model our output layer to have either a _sigmoid_ activation function or a _tanh_ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8) (768, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)\n",
    "# Our model is as follows\n",
    "network = Sequential()\n",
    "# adding the first layer\n",
    "network.add(Dense(12, input_dim=X.shape[-1], activation='relu'))\n",
    "# adding the second layer\n",
    "network.add(Dense(12, activation='relu'))\n",
    "# adding the output layer\n",
    "network.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our network has been setup, we can feed it data to train. We can divide our dataset into 2 parts, the training data and the test data. Even though _keras_ does this on its own, it is still a good practice when you have plentiful data.\n",
    "\n",
    "## Training the model\n",
    "\n",
    "We must specify the loss function to use to evaluate a set of weights, the optimizer used to search through different weights for the network and any optional metrics we would like to collect and report during training.\n",
    "\n",
    "In this case, we will use logarithmic loss, which for a binary classification problem is defined in Keras as “binary_crossentropy“. We will also use the efficient gradient descent algorithm “adam” for no other reason that it is an efficient default. Learn more about the Adam optimization algorithm in the paper “Adam: A Method for Stochastic Optimization“.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = X[:int(0.6 * X.shape[0])]\n",
    "training_y = y[:int(0.6 * y.shape[0])]\n",
    "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "iterations = int(150)\n",
    "# the batch size is used for the mini-batch grad descent which almost always \n",
    "# fastens the progress\n",
    "batch_size =  int(0.2 * X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea19c99710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(training_data, training_y, verbose=0, epochs=iterations, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking accuracy & Improvement\n",
    "\n",
    "Now that we have trained the model, we can check it's accuracy on the training set, test set, as well as the entire dataset. A good accuracy is around 80%+, but this can vary greatly according to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss loss \n",
      "Accuracy :  acc\n",
      "[('test_data', 66.449511478312246), ('training_data', 70.434782556865528), ('dataset', 68.880208333333343)]\n"
     ]
    }
   ],
   "source": [
    "# adding the scores\n",
    "import json\n",
    "scores = {\n",
    "    'test_data' : network.evaluate(X[-int(0.4 * X.shape[0]):], y[-int(0.4 * y.shape[0]):], verbose=0),\n",
    "    'training_data' : network.evaluate(training_data, training_y, verbose=0),\n",
    "    'dataset' : network.evaluate(X, y, verbose=0)\n",
    "}\n",
    "print('loss', network.metrics_names[0], '\\nAccuracy : ', network.metrics_names[1])\n",
    "print([(x, scores[x][1] * 100) for x in scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, our accuracy isn't that great. One thing we can do is normalize the data and perhaps use the _tanh_ activation function instead of the _sigmoid_ function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea184cdf60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = (training_data - np.mean(training_data, 0)) / np.std(training_data, 0)\n",
    "network.fit(training_data, training_y, epochs=iterations, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss loss \n",
      "Accuracy :  acc\n",
      "[('test_data', 76.221498313089924), ('training_data', 74.130434730778575), ('dataset', 74.869791666666657)]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 277\n",
      "Trainable params: 277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Final network\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "X = (X - np.mean(X, 0)) / np.std(X, 0)\n",
    "exec(In[7])\n",
    "print(\"Final network\\n\", network.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by normalizing the data, you see we get a huge improvement. On the whole, our accuracy has jumped from _64%_ to _78%_, a whopping **increase of 12%**. We can further improve this accuracy by:\n",
    "- adding more nodes to a layer\n",
    "- adding more layers\n",
    "- using _tanh_ activations, etc.\n",
    "\n",
    "Hyperparameters can be changed according to your use case and different settings may work differently for different people."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
