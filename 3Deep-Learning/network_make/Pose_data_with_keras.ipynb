{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Pose Data with Keras\n",
    "\n",
    "This is a collection of findings on modeling 3D point clouds with keras. All data is raw (_until processed_) and obtained using a Kinect sensor. A standard C# library obtains joint-point clouds which can be worked with.\n",
    "\n",
    "## Introduction & Specific Case\n",
    "\n",
    "In this situation, I will be trying to predict _yoga poses_, specifically the poses concerning **suryanamaskaar**. First, let us load the initial. Note that the data is already _pickled_ and just has to be loaded from the concerned files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((161, 77), (161, 1), (25,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "filename_X = 'X.pkl'\n",
    "filename_y = 'y.pkl'\n",
    "filename_labels = 'labels.pkl'\n",
    "all_labels = pickle.load(open(filename_labels, 'rb'))\n",
    "labels = np.array(all_labels['old_labels'])\n",
    "X = pickle.load(open(filename_X, 'rb'))\n",
    "y = pickle.load(open(filename_y, 'rb'))\n",
    "X.shape, y.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Pruning\n",
    "\n",
    "Herein, the data contains multiple joints and other details that can be pruned according to our use case. In this situation, analysis of the diagrams has shown that some joints are prone more to occlusion and can affect accuracy hence can be removed. In addition to this, the data can be normalized for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the last two angles\n",
    "X = X[:, :-2]\n",
    "# applying normalization\n",
    "X = (X - np.mean(X, 0)) / np.std(X, 0)\n",
    "assert(X.shape[0] == y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full sample space [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0]\n"
     ]
    }
   ],
   "source": [
    "poses = []\n",
    "for x in y:\n",
    "    if x[0, 0] not in poses:\n",
    "        poses.append(x[0, 0])\n",
    "print(\"Full sample space\", sorted(poses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the list of poses, a few of the poses are repeated, ie, output, say _5.0_ and _3.0_ refer to the same pose. Let us reduce the y dataset size by mapping the larger sample space to a reduced space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full sample space [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n"
     ]
    }
   ],
   "source": [
    "mapping = { 12: 1, 10: 3, 9: 4, 8: 5, 11: 8 }\n",
    "for ex in y:\n",
    "    try:\n",
    "        ex[0, 0] = mapping[ex[0, 0]]\n",
    "    except KeyError:\n",
    "        pass\n",
    "exec(In[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data finalization\n",
    "\n",
    "Now that we have pruned our data, let us recap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Dimens  (161, 75)\n",
      "y dimens  (161, 1)\n",
      "{12: 1, 10: 3, 9: 4, 8: 5, 11: 8}\n",
      "Data labels ['head' 'AnkleL' 'AnkleR' 'ElbowL' 'ElbowR' 'FootL' 'FootR' 'HandL' 'HandR'\n",
      " 'HandTipL' 'HandTipR' 'HipL' 'HipR' 'KneeL' 'KneeR' 'Neck' 'ShoulderL'\n",
      " 'ShoulderR' 'SpineBase' 'SpineMid' 'SpineShoulder' 'ThumbL' 'ThumbR'\n",
      " 'WristL' 'WristR']\n"
     ]
    }
   ],
   "source": [
    "print(\"X Dimens \", X.shape)\n",
    "print(\"y dimens \", y.shape)\n",
    "print(mapping)\n",
    "print(\"Data labels\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Pose Estimation process\n",
    "\n",
    "Now that we have our data cleaned, we can proceed to training the actual model and getting some test data. We will use _Keras_ with a _tensorflow_ backend. We can train different models to get the best perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# this will be the store for all the models\n",
    "models = []\n",
    "# import necessary keras packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we will be training is a simple model with **reLu** activation functions and single node output. This model will have 3 hidden layers with a _X.shape * 2_, _X.shape_, _X.shape * 2_, network layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 34,276\n",
      "Trainable params: 34,276\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_model = Sequential()\n",
    "simple_model.add(Dense(X.shape[-1] * 2, input_dim=X.shape[-1], activation='relu'))\n",
    "simple_model.add(Dense(X.shape[-1], activation='relu'))\n",
    "simple_model.add(Dense(X.shape[-1] * 2, activation='relu'))\n",
    "# adding the final output node\n",
    "simple_model.add(Dense(1, activation='sigmoid'))\n",
    "# compiling the made model\n",
    "from keras.metrics import binary_accuracy\n",
    "simple_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', binary_accuracy])\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to scale our y = data for this model, since it's output function is a sigmoid function which returns values between [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_X = X\n",
    "model_y = y / 8\n",
    "test_size = int( (20 / 100 ) * X.shape[0])\n",
    "training_size = int(X.shape[0] - test_size)\n",
    "batch_size = int(0.2 * training_size)\n",
    "iterations = int(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c5813aef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.fit(model_X[:training_size], model_y[:training_size], verbose=0, epochs=iterations, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has been trained, time to do some analytics. We have to analyse the performance on the test set, training set, as well as the entire dataset. Let us define a function to do this testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(test_model, X, y, training_set_size, test_set_size):\n",
    "    scores = {\n",
    "        '0test_types' : test_model.metrics_names,\n",
    "        '1training_data' : test_model.evaluate(X[:training_set_size], y[:training_set_size], verbose=0),\n",
    "        '2test_data' : test_model.evaluate(X[-test_set_size:], y[-test_set_size:], verbose=0),\n",
    "        '3dataset' : test_model.evaluate(X, y, verbose=0)\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0test_types': ['loss', 'acc', 'binary_accuracy'],\n",
       " '1training_data': [0.51429547750672633,\n",
       "  0.093023255813953487,\n",
       "  0.093023255813953487],\n",
       " '2test_data': [0.51569056510925293, 0.09375, 0.09375],\n",
       " '3dataset': [0.51457276773748928, 0.093167701863354033, 0.093167701863354033]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data(simple_model, model_X, model_y, training_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen the accuracy isn't that great, average around _9 - 10%_ irrespective of the dataset size. One thing we can do is convert the y label into a 8 feature vector and set the last function to be a **sigmoid activation** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "new_y = np.zeros((X.shape[0], len(poses)))\n",
    "for i, ex in enumerate(y):\n",
    "    new_y[i, int(ex[0, 0] - 1)] = 1\n",
    "print(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 1208      \n",
      "=================================================================\n",
      "Total params: 35,333\n",
      "Trainable params: 35,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = Sequential()\n",
    "new_model.add(Dense(X.shape[-1] * 2, input_dim=X.shape[-1], activation='relu'))\n",
    "new_model.add(Dense(X.shape[-1], activation='relu'))\n",
    "new_model.add(Dense(X.shape[-1] * 2, activation='relu'))\n",
    "new_model.add(Dense(len(poses), activation='sigmoid'))\n",
    "new_model.summary()\n",
    "new_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c4f6c3be0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit(X[:training_size], new_y[:training_size], verbose=0, epochs=iterations, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0test_types': ['loss', 'acc', 'binary_accuracy'],\n",
       " '1training_data': [1.0240115966200759e-07, 1.0, 1.0],\n",
       " '2test_data': [0.18188676238059998, 0.984375, 0.984375],\n",
       " '3dataset': [0.036151491197304063, 0.99689440993788825, 0.99689440993788825]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data(new_model, X, new_y, training_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, just by changing our output array to become a _binary vector_ which stores probability of pose, we have gotten a *tremendous increase*, all the way to **93 - 96%** across the board. Is is evident that our new model is much better than the previous one. \n",
    "\n",
    "## Testing for errors\n",
    "\n",
    "Now let us test the examples manually to find patterns in the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 [WRONG]  1 != 2.0\n",
      "144 [WRONG]  4 != 2.0\n",
      "159 [WRONG]  5 != 7.0\n"
     ]
    }
   ],
   "source": [
    "# function to get easy prediction format\n",
    "def get_prediction(X, i):\n",
    "    f = np.ndarray.tolist(new_model.predict(X[i]))[0]\n",
    "    return f.index(max(f)) + 1\n",
    "import random\n",
    "predictions = [(get_prediction(X, i), y[i, 0]) for i in range(X.shape[0])]\n",
    "for i, (predicted, actual) in enumerate(predictions):\n",
    "    if predicted != actual:\n",
    "        print(\"%d [WRONG] \" % i, predicted, \"!=\", actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this model is pretty satisfactory, we can save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "j = json.loads(new_model.to_json())\n",
    "json.dump(j, open('trained_model.json', 'w'))\n",
    "new_model.save('trained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with new data \n",
    "\n",
    "Now that the model has been trained, we can proceed to test it with new data. Let us load the model from the file, and proceed. The data is stored in a pickled format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "try:\n",
    "    del new_model\n",
    "    del simple_model\n",
    "except:\n",
    "    pass\n",
    "newdata_file = 'dataset_new.pkl'\n",
    "\n",
    "cur_model = load_model('trained_model.h5')\n",
    "new_data = pickle.load(open(newdata_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 150)               11400     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 1208      \n",
      "=================================================================\n",
      "Total params: 35,333\n",
      "Trainable params: 35,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cur_model.summary()\n",
    "new_data = np.matrix(new_data)\n",
    "new_data = (new_data - np.mean(new_data, 0)) / np.std(new_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been normalized, we can try prediction of a few random poses. However, the data isn't sorted the same way the previous data was, so we will have to take care of that. All the sorting has been done by using the `pandas` module. The data has been pickled and copied to a file. Let us load the data from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headX</th>\n",
       "      <th>headY</th>\n",
       "      <th>headZ</th>\n",
       "      <th>AnkleLX</th>\n",
       "      <th>AnkleLY</th>\n",
       "      <th>AnkleLZ</th>\n",
       "      <th>AnkleRX</th>\n",
       "      <th>AnkleRY</th>\n",
       "      <th>AnkleRZ</th>\n",
       "      <th>ElbowLX</th>\n",
       "      <th>...</th>\n",
       "      <th>ThumbLZ</th>\n",
       "      <th>ThumbRX</th>\n",
       "      <th>ThumbRY</th>\n",
       "      <th>ThumbRZ</th>\n",
       "      <th>WristLX</th>\n",
       "      <th>WristLY</th>\n",
       "      <th>WristLZ</th>\n",
       "      <th>WristRX</th>\n",
       "      <th>WristRY</th>\n",
       "      <th>WristRZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.280684</td>\n",
       "      <td>0.988059</td>\n",
       "      <td>3.263694</td>\n",
       "      <td>0.254301</td>\n",
       "      <td>-0.444253</td>\n",
       "      <td>3.343855</td>\n",
       "      <td>0.338832</td>\n",
       "      <td>-0.448342</td>\n",
       "      <td>3.342944</td>\n",
       "      <td>0.059723</td>\n",
       "      <td>...</td>\n",
       "      <td>3.154000</td>\n",
       "      <td>0.526344</td>\n",
       "      <td>0.163979</td>\n",
       "      <td>3.212111</td>\n",
       "      <td>0.060925</td>\n",
       "      <td>0.271715</td>\n",
       "      <td>3.224301</td>\n",
       "      <td>0.527751</td>\n",
       "      <td>0.265957</td>\n",
       "      <td>3.274888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.281372</td>\n",
       "      <td>0.987621</td>\n",
       "      <td>3.267906</td>\n",
       "      <td>0.255337</td>\n",
       "      <td>-0.442164</td>\n",
       "      <td>3.346285</td>\n",
       "      <td>0.342003</td>\n",
       "      <td>-0.443822</td>\n",
       "      <td>3.348526</td>\n",
       "      <td>0.063121</td>\n",
       "      <td>...</td>\n",
       "      <td>3.158571</td>\n",
       "      <td>0.522428</td>\n",
       "      <td>0.176023</td>\n",
       "      <td>3.200111</td>\n",
       "      <td>0.066423</td>\n",
       "      <td>0.269686</td>\n",
       "      <td>3.223008</td>\n",
       "      <td>0.522764</td>\n",
       "      <td>0.277916</td>\n",
       "      <td>3.266220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.282592</td>\n",
       "      <td>0.988507</td>\n",
       "      <td>3.273247</td>\n",
       "      <td>0.257084</td>\n",
       "      <td>-0.437238</td>\n",
       "      <td>3.348634</td>\n",
       "      <td>0.343773</td>\n",
       "      <td>-0.441450</td>\n",
       "      <td>3.351580</td>\n",
       "      <td>0.066551</td>\n",
       "      <td>...</td>\n",
       "      <td>3.180250</td>\n",
       "      <td>0.524024</td>\n",
       "      <td>0.192591</td>\n",
       "      <td>3.208750</td>\n",
       "      <td>0.074040</td>\n",
       "      <td>0.280027</td>\n",
       "      <td>3.225738</td>\n",
       "      <td>0.521314</td>\n",
       "      <td>0.278699</td>\n",
       "      <td>3.263463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.284382</td>\n",
       "      <td>0.990231</td>\n",
       "      <td>3.278028</td>\n",
       "      <td>0.257059</td>\n",
       "      <td>-0.437400</td>\n",
       "      <td>3.349086</td>\n",
       "      <td>0.343529</td>\n",
       "      <td>-0.441425</td>\n",
       "      <td>3.351696</td>\n",
       "      <td>0.070192</td>\n",
       "      <td>...</td>\n",
       "      <td>3.174875</td>\n",
       "      <td>0.514929</td>\n",
       "      <td>0.196622</td>\n",
       "      <td>3.204286</td>\n",
       "      <td>0.079813</td>\n",
       "      <td>0.286167</td>\n",
       "      <td>3.227628</td>\n",
       "      <td>0.519758</td>\n",
       "      <td>0.277824</td>\n",
       "      <td>3.262696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.285024</td>\n",
       "      <td>0.990090</td>\n",
       "      <td>3.278895</td>\n",
       "      <td>0.256005</td>\n",
       "      <td>-0.438547</td>\n",
       "      <td>3.349479</td>\n",
       "      <td>0.343443</td>\n",
       "      <td>-0.441609</td>\n",
       "      <td>3.351665</td>\n",
       "      <td>0.075397</td>\n",
       "      <td>...</td>\n",
       "      <td>3.217080</td>\n",
       "      <td>0.519769</td>\n",
       "      <td>0.213355</td>\n",
       "      <td>3.213834</td>\n",
       "      <td>0.087647</td>\n",
       "      <td>0.288218</td>\n",
       "      <td>3.229130</td>\n",
       "      <td>0.518375</td>\n",
       "      <td>0.278476</td>\n",
       "      <td>3.260859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.288270</td>\n",
       "      <td>0.991880</td>\n",
       "      <td>3.282324</td>\n",
       "      <td>0.252805</td>\n",
       "      <td>-0.440628</td>\n",
       "      <td>3.351120</td>\n",
       "      <td>0.344895</td>\n",
       "      <td>-0.435369</td>\n",
       "      <td>3.354405</td>\n",
       "      <td>0.078418</td>\n",
       "      <td>...</td>\n",
       "      <td>3.196308</td>\n",
       "      <td>0.518031</td>\n",
       "      <td>0.202581</td>\n",
       "      <td>3.211932</td>\n",
       "      <td>0.108080</td>\n",
       "      <td>0.250589</td>\n",
       "      <td>3.224464</td>\n",
       "      <td>0.518165</td>\n",
       "      <td>0.278385</td>\n",
       "      <td>3.260657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.290292</td>\n",
       "      <td>0.991635</td>\n",
       "      <td>3.283504</td>\n",
       "      <td>0.253375</td>\n",
       "      <td>-0.446780</td>\n",
       "      <td>3.348309</td>\n",
       "      <td>0.344739</td>\n",
       "      <td>-0.435814</td>\n",
       "      <td>3.353754</td>\n",
       "      <td>0.080520</td>\n",
       "      <td>...</td>\n",
       "      <td>3.191308</td>\n",
       "      <td>0.518552</td>\n",
       "      <td>0.228914</td>\n",
       "      <td>3.229000</td>\n",
       "      <td>0.100447</td>\n",
       "      <td>0.287098</td>\n",
       "      <td>3.232600</td>\n",
       "      <td>0.517889</td>\n",
       "      <td>0.278280</td>\n",
       "      <td>3.260650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.294620</td>\n",
       "      <td>0.991994</td>\n",
       "      <td>3.287684</td>\n",
       "      <td>0.254559</td>\n",
       "      <td>-0.441441</td>\n",
       "      <td>3.350679</td>\n",
       "      <td>0.344636</td>\n",
       "      <td>-0.435997</td>\n",
       "      <td>3.353716</td>\n",
       "      <td>0.081978</td>\n",
       "      <td>...</td>\n",
       "      <td>3.197600</td>\n",
       "      <td>0.516335</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>3.215000</td>\n",
       "      <td>0.107225</td>\n",
       "      <td>0.267964</td>\n",
       "      <td>3.234044</td>\n",
       "      <td>0.517034</td>\n",
       "      <td>0.278482</td>\n",
       "      <td>3.261563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.295628</td>\n",
       "      <td>0.991814</td>\n",
       "      <td>3.287842</td>\n",
       "      <td>0.253678</td>\n",
       "      <td>-0.440837</td>\n",
       "      <td>3.351065</td>\n",
       "      <td>0.342701</td>\n",
       "      <td>-0.437185</td>\n",
       "      <td>3.353600</td>\n",
       "      <td>0.082893</td>\n",
       "      <td>...</td>\n",
       "      <td>3.201401</td>\n",
       "      <td>0.520284</td>\n",
       "      <td>0.148180</td>\n",
       "      <td>3.198500</td>\n",
       "      <td>0.106880</td>\n",
       "      <td>0.267847</td>\n",
       "      <td>3.237896</td>\n",
       "      <td>0.516086</td>\n",
       "      <td>0.278092</td>\n",
       "      <td>3.262226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.297428</td>\n",
       "      <td>0.991606</td>\n",
       "      <td>3.288206</td>\n",
       "      <td>0.253852</td>\n",
       "      <td>-0.440620</td>\n",
       "      <td>3.351242</td>\n",
       "      <td>0.342007</td>\n",
       "      <td>-0.438000</td>\n",
       "      <td>3.352500</td>\n",
       "      <td>0.084353</td>\n",
       "      <td>...</td>\n",
       "      <td>3.205637</td>\n",
       "      <td>0.516971</td>\n",
       "      <td>0.171947</td>\n",
       "      <td>3.216100</td>\n",
       "      <td>0.103504</td>\n",
       "      <td>0.284026</td>\n",
       "      <td>3.257442</td>\n",
       "      <td>0.513883</td>\n",
       "      <td>0.276644</td>\n",
       "      <td>3.265942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      headX     headY     headZ   AnkleLX   AnkleLY   AnkleLZ   AnkleRX  \\\n",
       "0  0.280684  0.988059  3.263694  0.254301 -0.444253  3.343855  0.338832   \n",
       "1  0.281372  0.987621  3.267906  0.255337 -0.442164  3.346285  0.342003   \n",
       "2  0.282592  0.988507  3.273247  0.257084 -0.437238  3.348634  0.343773   \n",
       "3  0.284382  0.990231  3.278028  0.257059 -0.437400  3.349086  0.343529   \n",
       "4  0.285024  0.990090  3.278895  0.256005 -0.438547  3.349479  0.343443   \n",
       "5  0.288270  0.991880  3.282324  0.252805 -0.440628  3.351120  0.344895   \n",
       "6  0.290292  0.991635  3.283504  0.253375 -0.446780  3.348309  0.344739   \n",
       "7  0.294620  0.991994  3.287684  0.254559 -0.441441  3.350679  0.344636   \n",
       "8  0.295628  0.991814  3.287842  0.253678 -0.440837  3.351065  0.342701   \n",
       "9  0.297428  0.991606  3.288206  0.253852 -0.440620  3.351242  0.342007   \n",
       "\n",
       "    AnkleRY   AnkleRZ   ElbowLX    ...      ThumbLZ   ThumbRX   ThumbRY  \\\n",
       "0 -0.448342  3.342944  0.059723    ...     3.154000  0.526344  0.163979   \n",
       "1 -0.443822  3.348526  0.063121    ...     3.158571  0.522428  0.176023   \n",
       "2 -0.441450  3.351580  0.066551    ...     3.180250  0.524024  0.192591   \n",
       "3 -0.441425  3.351696  0.070192    ...     3.174875  0.514929  0.196622   \n",
       "4 -0.441609  3.351665  0.075397    ...     3.217080  0.519769  0.213355   \n",
       "5 -0.435369  3.354405  0.078418    ...     3.196308  0.518031  0.202581   \n",
       "6 -0.435814  3.353754  0.080520    ...     3.191308  0.518552  0.228914   \n",
       "7 -0.435997  3.353716  0.081978    ...     3.197600  0.516335  0.197266   \n",
       "8 -0.437185  3.353600  0.082893    ...     3.201401  0.520284  0.148180   \n",
       "9 -0.438000  3.352500  0.084353    ...     3.205637  0.516971  0.171947   \n",
       "\n",
       "    ThumbRZ   WristLX   WristLY   WristLZ   WristRX   WristRY   WristRZ  \n",
       "0  3.212111  0.060925  0.271715  3.224301  0.527751  0.265957  3.274888  \n",
       "1  3.200111  0.066423  0.269686  3.223008  0.522764  0.277916  3.266220  \n",
       "2  3.208750  0.074040  0.280027  3.225738  0.521314  0.278699  3.263463  \n",
       "3  3.204286  0.079813  0.286167  3.227628  0.519758  0.277824  3.262696  \n",
       "4  3.213834  0.087647  0.288218  3.229130  0.518375  0.278476  3.260859  \n",
       "5  3.211932  0.108080  0.250589  3.224464  0.518165  0.278385  3.260657  \n",
       "6  3.229000  0.100447  0.287098  3.232600  0.517889  0.278280  3.260650  \n",
       "7  3.215000  0.107225  0.267964  3.234044  0.517034  0.278482  3.261563  \n",
       "8  3.198500  0.106880  0.267847  3.237896  0.516086  0.278092  3.262226  \n",
       "9  3.216100  0.103504  0.284026  3.257442  0.513883  0.276644  3.265942  \n",
       "\n",
       "[10 rows x 75 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datafile = 'all_data_X.pkl'\n",
    "old_dataset = pd.read_pickle('old_data.pkl')\n",
    "new_dataset = pd.read_pickle('new_data.pkl')\n",
    "new_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 939, 2: 458, 3: 62, 4: 720, 5: 500, 6: 129, 7: 54, 8: 1},\n",
       " 'Classified : ',\n",
       " 87.92997542997543)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = np.matrix(new_dataset.get_values())\n",
    "new_data = (new_data - np.mean(new_data, 0)) / np.std(new_data, 0)\n",
    "\n",
    "# setup pose storing data\n",
    "poses = {}\n",
    "pose_indices = {}\n",
    "for _ in range(1, 9):\n",
    "    poses[_] = 0\n",
    "    pose_indices[_] = []\n",
    "\n",
    "# storing the poses and other data    \n",
    "for ind, _ in enumerate(new_data):\n",
    "    prediction = cur_model.predict(_)\n",
    "    if np.any(prediction > 9e-2) and np.max(prediction) > 0.5:\n",
    "            prediction = np.ndarray.tolist(prediction)\n",
    "            pose_ind = prediction[-1].index(max(prediction[-1])) + 1\n",
    "            pose_indices[pose_ind].append(ind)\n",
    "            poses[pose_ind] += 1\n",
    "\n",
    "poses, \"Classified : \", sum([poses[_] for _ in poses]) / new_data.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying the estimated poses\n",
    "\n",
    "Now that out data has been collected, we can randomly visualize some of the predicted poses to make sure they have been classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def draw_person(person, pose_no, labels):\n",
    "\tplt.clf()\n",
    "\tfig = plt.figure()\n",
    "\tax = fig.gca(projection = '3d')\n",
    "\tfor _ in range(person.shape[0]):\n",
    "\t\tax.scatter(person[_, 0], person[_, 2],person[_, 1])\n",
    "\t\tax.text(person[_, 0], person[_, 2], person[_, 1], labels[_],\\\n",
    "\t\t size = 12, zorder = 1)\n",
    "\tax.set_title(pose_no)\n",
    "\tax.set_ylim(1, 3)\n",
    "\tplt.show()\n",
    "\n",
    "def get_person(ind, dataset):\n",
    "    return dataset[ind].reshape(25, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "# visualize one pose for every type of pose\n",
    "for i in range(1, 2):\n",
    "    ind = random.choice(pose_indices[i])\n",
    "    person = get_person(ind, new_data)\n",
    "    draw_person(person, i, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the visualizations, the poses have been classified correctly and now we can further train the model to improve it's accuracy.\n",
    "\n",
    "## Adding second round of training\n",
    "\n",
    "To add a second round of training, all we need to do is setup the corresponding _y vectors_ for the correctly classified new poses, and run another `model.fit`, after which we can store the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2863, 8), (2863, 75))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first creating the y vector\n",
    "newdata_y = []\n",
    "labeled_x = []\n",
    "for i in pose_indices:\n",
    "    y_vec = np.zeros((8, 1))\n",
    "    y_vec[i - 1, 0] = 1\n",
    "    for x_vals in pose_indices[i]:\n",
    "        labeled_x.append(new_data[x_vals])\n",
    "        newdata_y.append(y_vec)\n",
    "new_train_labels = np.array(newdata_y).reshape(len(newdata_y), len(newdata_y[0]))\n",
    "new_train_X = np.array(labeled_x).reshape(len(labeled_x), new_data.shape[-1])\n",
    "new_train_labels.shape, new_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = int(0.6 * new_train_labels.shape[0])\n",
    "test_size = new_train_labels.shape[0] - training_size\n",
    "iterations = int(1e4)\n",
    "batch_size = int(0.05 * new_train_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permuting the data before training\n",
    "random_order = np.random.permutation(new_train_X.shape[0])\n",
    "new_train_X = new_train_X[random_order]\n",
    "new_train_labels = new_train_labels[random_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c4f6c3908>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_model.fit(new_train_X[:training_size], new_train_labels[:training_size], verbose=0, epochs=iterations, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing (Once Again!)\n",
    "\n",
    "Now that the new model has been trained, let us shuffle the data and test the model. First we will test on the current large dataset and then test on the original X, y dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_order = np.random.permutation(new_train_X.shape[0])\n",
    "new_train_X = new_train_X[random_order]\n",
    "new_train_labels = new_train_labels[random_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0test_types': ['loss', 'acc', 'binary_accuracy'],\n",
       " '1training_data': [0.022695497494558958,\n",
       "  0.99803436225975539,\n",
       "  0.99803436225975539],\n",
       " '2test_data': [0.032186949419580918,\n",
       "  0.99705497382198949,\n",
       "  0.99705497382198949],\n",
       " '3dataset': [0.026494730274943611, 0.99764233321690532, 0.99764233321690532]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data(cur_model, new_train_X, new_train_labels, training_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it on the X, y original dataset, after perumutation we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_order = np.random.permutation(X.shape[0])\n",
    "X = X[random_order]\n",
    "new_y = new_y[random_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0test_types': ['loss', 'acc', 'binary_accuracy'],\n",
       " '1training_data': [1.6491459608078003, 0.88671875, 0.88671875],\n",
       " '2test_data': [1.3273274950962861, 0.90576923076923077, 0.90576923076923077],\n",
       " '3dataset': [1.5192192510484934, 0.89440993788819878, 0.89440993788819878]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size = int(0.6 * X.shape[0])\n",
    "test_size = X.shape[0] - training_size\n",
    "test_data(cur_model, X, new_y, training_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving information\n",
    "\n",
    "Finally, all the relevant information can be saved for future use. This includes the trained model, the new X and y data along with the old one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model with the weights, optimizer etc\n",
    "cur_model.save('trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2863, 75), (2863, 8))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, new_y.shape, new_train_X.shape, new_train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make the complete x dataset\n",
    "complete_dataset = pd.DataFrame(new_train_X, columns=old_dataset.columns).append(old_dataset)\n",
    "assert(complete_dataset.shape[0] == old_dataset.shape[0] + new_train_X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the complete y dataset\n",
    "y_labels = pd.DataFrame(new_train_labels, columns=[_ for _ in range(1, 9)])\n",
    "new_y_labels = pd.DataFrame(new_y, columns = [_ for _ in range(1, 9)])\n",
    "complete_y = y_labels.append(new_y_labels)\n",
    "assert(complete_y.shape[0] == new_y.shape[0] + new_train_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3024, 3024)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_y.shape[0], complete_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset[complete_y.columns] = complete_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset.to_pickle('complete_dataset.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
