{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot using `tflearn` and `tensorflow`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is a pretty basic chatbot using tensorflow and [`tflearn`](http://tflearn.org/getting_started/). Basic data classification into _intents_. After identifying the intent of a query, an appropriate response can be generated.\n",
    "This document will cover all the basics I've gathered around the internet, but mainly from [this](https://chatbotsmagazine.com/contextual-chat-bots-with-tensorflow-4391749d0077) article.\n",
    "Each intent will have these attributes:\n",
    "- A tag identifier\n",
    "- Patterns (ways in which the input will/can be modeled\n",
    "- Responses (ways in which our bot will respond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the basics required\n",
    "\n",
    "# parsing the data \n",
    "import numpy as np\n",
    "\n",
    "# libraries required for learning\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the data and making the corpus\n",
    "\n",
    "The first step is to classify our intents and get the _preprocessing_ out of the way. Our intent patterns have to be \"stemmed\", i.e, words which convey the same base, have to map to the same pattern for a given intent. _For example_, having, have, etc should all map to have, since it is the **common base**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"context_filter\": [\n",
      "      \"rentquery\",\n",
      "      \"rental\"\n",
      "    ],\n",
      "    \"patterns\": [\n",
      "      \"Can we rent a moped?\",\n",
      "      \"I'd like to rent a moped\",\n",
      "      \"How does this work?\"\n",
      "    ],\n",
      "    \"responses\": [\n",
      "      \"Are you looking to rent today or later this week?\"\n",
      "    ],\n",
      "    \"set_context\": [\n",
      "      \"rentalday\",\n",
      "      \"today\"\n",
      "    ],\n",
      "    \"tag\": \"rental\"\n",
      "  },\n",
      "  {\n",
      "    \"context_filter\": [\n",
      "      \"rentalday\",\n",
      "      \"today\"\n",
      "    ],\n",
      "    \"patterns\": [\n",
      "      \"today\"\n",
      "    ],\n",
      "    \"responses\": [\n",
      "      \"For rentals today please call 1-800-MYMOPED\",\n",
      "      \"Same-day rentals please call 1-800-MYMOPED\"\n",
      "    ],\n",
      "    \"set_context\": [\n",
      "      \"rentquery\"\n",
      "    ],\n",
      "    \"tag\": \"today\"\n",
      "  }\n",
      "] ...\n"
     ]
    }
   ],
   "source": [
    "# import the data/intents\n",
    "import json\n",
    "intent_file = \"intents.json\"\n",
    "intents = json.load(open(intent_file))\n",
    "for intent in intents:\n",
    "    try:\n",
    "        current_context = intent['context_filter']\n",
    "        current_context.append(intent['tag'])\n",
    "    except KeyError:\n",
    "        current_context = [\"\"]\n",
    "    intent['context_filter'] = list(set(current_context))\n",
    "    try:\n",
    "        future_context = intent['set_context']\n",
    "    except KeyError:\n",
    "        future_context = [\"\"]\n",
    "    intent['set_context'] = list(set(future_context))\n",
    "json.dump(intents, open(intent_file, 'w+'))\n",
    "print(json.dumps(intents[-2:], indent=2, sort_keys=True), \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the basic data, we have to stem it, and separate it into _classes_ (essentially intent-identifiers), optionally ignoring common words like articles (the, are, etc). Do note, that pronouns might seem redundant or unecessary but more often that not they are important to indentify the target for the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pratik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the necessary language processing tools\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words :  {'thank', 'kind', 'lik', 'what', 'doe', 'hour', \"'s\", 'which', 'day', 'do', 'i', 'a', 'acceiv', 'mop', 'thi', 'work', 'goodby', 'anyon', 'yo', 'when', 'help', 'hi', 'today', 'how', 'lat', 'cash', 'op', 'can', 'rent', 'bye', 'card', 'that', 'ar', 'good', 'see', 'we', 'hav', 'to', 'is', 'mastercard', 'tak', 'on', 'ther', 'credit', 'of', \"'d\", 'you', 'hello'} \n",
      "\n",
      "classes :  ['hours', 'goodbye', 'thanks', 'mopeds', 'greeting', 'payments', 'opentoday', 'today', 'rental'] \n",
      "\n",
      "documents :  [(['hi'], 'greeting'), (['how', 'ar', 'you'], 'greeting'), (['is', 'anyon', 'ther'], 'greeting'), (['hello'], 'greeting'), (['good', 'day'], 'greeting')] ...\n"
     ]
    }
   ],
   "source": [
    "words, classes, documents = set(), set(), []\n",
    "# usually stop words should/can be removed, but phrase searching becomes difficult\n",
    "# can remove stop words for large datasets\n",
    "ignore_words = set(['?'])\n",
    "for intent in intents:\n",
    "    # add the relevant class\n",
    "    classes.add(intent['tag'])\n",
    "    # stem the words in the patterns and add it to the document set\n",
    "    for pattern in intent['patterns']:\n",
    "        current_words = [stemmer.stem(_.lower()) for _ in nltk.word_tokenize(pattern) if _ not in ignore_words]\n",
    "        [words.add(_) for _ in current_words]\n",
    "        documents.append((current_words, intent['tag']))\n",
    "classes = list(classes)\n",
    "print(\"words : \", words, \"\\n\\nclasses : \", classes, \"\\n\\ndocuments : \", documents[:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our words stemmed and classified into documents, we can start training the model. First, we have to convert the data into a tensor (this is mainly done for speed and compatibility, you'd have to code a network from scratch/use another package to work with raw data). The bag here basically is a vector that contains the truth value of whether the word in our words is present in the given pattern. _for example_, if my complete wordset is `[\"how\", \"are\", \"you\", \"doing\"]` and my current pattern is `[\"how\"]`, then my word _bag_ will be `[1, 0, 0, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples :  27 \n",
      "x_i :  48 \n",
      "y_i :  9\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "y_labels = []\n",
    "import random\n",
    "for pattern, tag in documents:\n",
    "    bag = [ 1 if _ in pattern else 0 for _ in words]\n",
    "    y_label = [0] * len(classes)\n",
    "    y_label[classes.index(tag)] = 1\n",
    "    dataset.append((bag, y_label))\n",
    "# shuffle the dataset to eliminate biases\n",
    "random.shuffle(dataset)\n",
    "print(\"Training examples : \", len(dataset), \"\\nx_i : \", len(dataset[0][0]), \"\\ny_i : \", len(dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Now that our basic dataset is ready, all that's left is to train the model on it. Given that we don't have a lage number of classes and only a few training examples, this step wouldn't take a lot of time. But for large corpuses, this can be the longest step, so make sure you have _good_ hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network\n",
    "training_ex = random.choice(dataset)\n",
    "x_i, y_i = training_ex\n",
    "net = tflearn.input_data(shape=[None, len(x_i)])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(y_i), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# define the network model and log output\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the very little number of training examples, it would make no sense to split the data into _training_ and _test_. One could account the model's accuracy to overfitting, but given the shallow nature of the model, this is probably not the case. First, let us save the data and the model before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/home/pratik/0Coding/93Machine-Learning/4Chat-Bot/tflearn_chatbot/untrained_model is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model_data = {\n",
    "    'dataset' : dataset,\n",
    "    'words' : words,\n",
    "    'classes' : classes,\n",
    "}\n",
    "# save data\n",
    "pickle.dump(model_data, open('model_data.pkl', 'wb'))\n",
    "# save the model\n",
    "model.save('untrained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 11999  | total loss: \u001b[1m\u001b[32m0.19336\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4000 | loss: 0.19336 - acc: 0.9840 -- iter: 18/27\n",
      "Training Step: 12000  | total loss: \u001b[1m\u001b[32m1.38208\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 4000 | loss: 1.38208 - acc: 0.8967 -- iter: 27/27\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# gather the data\n",
    "train_X, train_y = np.array([_[0] for _ in dataset]), np.array([_[1] for _ in dataset])\n",
    "print(train_X.shape, train_y.shape)\n",
    "iterations = 4000\n",
    "batch = 9\n",
    "model.fit(train_X, train_y, n_epoch=iterations, batch_size=batch, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/home/pratik/0Coding/93Machine-Learning/4Chat-Bot/tflearn_chatbot/trained_model is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# we can save the trained model now\n",
    "model.save('trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the **chatbot**\n",
    "\n",
    "Now that our model has successfully been trained, we can setup the _actual_ chatbot framework. This would involve cleaning user inputs and modeling them into bags, and then predicting the intent using our model.\n",
    "\n",
    "### Wrapping the model\n",
    "\n",
    "The first step we need to cover is wrapping the `model.predict` such that I can create a black box, which, when given a sentence as input, returns the possible intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 48) [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# function to return the bag of words given an input sentence\n",
    "def getXlabel(sentence):\n",
    "    stemmed_words = [stemmer.stem(_.lower()) for _ in nltk.word_tokenize(sentence) if _ not in ignore_words]\n",
    "    return np.array([ [1] if _ in stemmed_words else [0] for _ in words]).T\n",
    "# example \n",
    "ex_sentence = \"How are you today?\"\n",
    "x_label = getXlabel(ex_sentence)\n",
    "print(x_label.shape, x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.54070007e-08,   6.09317445e-04,   8.89364421e-01,\n",
       "          1.30644266e-05,   3.34204957e-02,   8.52505707e-07,\n",
       "          4.98453915e-07,   1.22153084e-04,   7.64690563e-02]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"How are you today?\"\n",
    "model.predict(getXlabel(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the probability of the given setence having a particular intent. Let us wrap the prediction in a function that returns _either_ a given intent or `None` depending on the probability. The function I've modeled sets a threshold above which only the sentence is considered. Optionally, you can set `show_probability` to `True`, to get the probability of the return intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIntent(sentence, intents=model_data['classes'], threshold=0.7, show_probability=False):\n",
    "    intent = None\n",
    "    x_label = getXlabel(sentence)\n",
    "    prediction = np.ndarray.tolist(model.predict(x_label)[0])\n",
    "    # if no intent matches above the threshold\n",
    "    if all([_ < threshold for _ in prediction]):\n",
    "        if show_probability:\n",
    "            return None, 0\n",
    "        return None\n",
    "    try:\n",
    "        assert(len(intents) == len(prediction))\n",
    "    except AssertionError:\n",
    "        print(\"Number of intents do not match predictions\")\n",
    "        return False\n",
    "    intent = intents[prediction.index(max(prediction))]\n",
    "    if show_probability:\n",
    "        return intent, prediction[prediction.index(max(prediction))]\n",
    "    return intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('greeting', 0.8824049234390259)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example intent that does not exist in the corpus\n",
    "getIntent(\"hi, how are you doing today?\", show_probability=True, threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Awareness\n",
    "\n",
    "Now that we have a wrapper, it's time to move our focus onto the _contextual awareness_ of our bot. Contextual awareness implies the steps the bot should take in order to move the context forward (in this situation, the end goal would be do have a successful booking done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there, how can I help?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to generate a response on the basis on input\n",
    "min_prob = 0.8 # set this according to your results\n",
    "intent_responses = {}\n",
    "for _ in intents:\n",
    "    intent_responses[_['tag']] = _['responses']\n",
    "def getResponse(sentence):\n",
    "    intent, prob = getIntent(sentence, threshold=min_prob, show_probability=True)\n",
    "    if intent is None:\n",
    "        return (\"I'm having a hard time understanding, \"\n",
    "                \"could you please rephrase that\")\n",
    "    return random.choice(intent_responses[intent])\n",
    "getResponse(\"Hi, how are you today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to add a way for the conversation to continue forward. We can do this by linking the intents to one another. _For example_, if a user enters the keyword `today`, we have to return a response relevant in this context, basically, return work timings, **even if** the probability of that (_opentoday_) intent is lesser than the probability of _rentaltoday_, BECAUSE the context here is different. The first step here would be to modify our `getIntent` function to return a list of possible intents, and not _just_ the one with the maximum probability. (_note, for contextual intent recognition, we will have to set our threshold a little lower than before since patterns, that **may not** match the given sentence EXACTLY, may be the actual required response in the given context_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thanks', 0.69497228), ('rental', 0.30028212)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getIntent(sentence, intents=model_data['classes'], threshold=0.7):\n",
    "    prediction = model.predict(getXlabel(sentence))\n",
    "    # if no intent matches above the threshold\n",
    "    if np.all(prediction < threshold):\n",
    "        return [(None, 0)]\n",
    "    try:\n",
    "        assert(len(intents) == prediction.shape[1])\n",
    "    except AssertionError:\n",
    "        print(\"Number of intents do not match predictions\")\n",
    "        return False\n",
    "    intents = np.array(intents).reshape(1, len(intents))\n",
    "    intents = intents[prediction >= threshold]\n",
    "    results = [(x, y) for x, y in zip(intents, \n",
    "       prediction[prediction >= threshold])]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results\n",
    "getIntent(\"I wanted to book a moped\", threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we get a list of results, we can respond with a given response **iff** the context is relevant _OR_ when the response is of a _context-free_ intent, like a _greeting_. Thus a response is to be displayed only if _either_ it is context-free or the context of a request _matches_ it's context-filter. _Following will illustrate_:\n",
    "- Get user sentence\n",
    "- Make prediction\n",
    "- If current context is set:\n",
    "    - Iterate through results\n",
    "    - return `result` where `context == current_context`\n",
    "- If current context is not set:\n",
    "    - Iterate through results\n",
    "    - return `result` **if** `result` is _context-free_\n",
    "    - else, set_context if `result` has a context\n",
    "    - return `result`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this will be the context store for our user to keep track of conversations\n",
    "context = [[]]\n",
    "# tagged intents for easy intent finding\n",
    "tagged_intents = {}\n",
    "for _ in intents:\n",
    "    tagged_intents[_['tag']] = _\n",
    "    \n",
    "# this getResponse function is now modifying to take care of state as well\n",
    "def getResponse(sentence, verbose=False, threshold=0.5):\n",
    "    results = getIntent(sentence, threshold=threshold)\n",
    "    is_first = True\n",
    "    # set the default response in case of no predictions\n",
    "    response = (\"I'm having a hard time understanding, \"\n",
    "                \"could you please rephrase that :)\")\n",
    "    if verbose:\n",
    "        print(\"Matches : \", results)\n",
    "    # decreasing order of probability\n",
    "    for result in results:\n",
    "        intent, prob = result\n",
    "        matched_intent = tagged_intents[intent]\n",
    "        if verbose:\n",
    "            print(\"# : \", json.dumps(matched_intent, indent=4))\n",
    "            \n",
    "        # in case the best response hasn't been set\n",
    "        if is_first: # and \"\" in matched_intent['set_context']:\n",
    "            response = random.choice(matched_intent['responses'])\n",
    "            context.append(matched_intent['set_context'])\n",
    "            is_first = False\n",
    "            continue\n",
    "            \n",
    "        # if context of matched_intent matches previously set context\n",
    "        # return immediately\n",
    "        if any([_ for _ in context[-2 + 1 * is_first] if _ in matched_intent['context_filter']]):\n",
    "            response = random.choice(matched_intent['responses'])\n",
    "            if is_first:\n",
    "                context.append(matched_intent['set_context'])\n",
    "            else:\n",
    "                context[-1] = matched_intent['set_context']\n",
    "            break\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the responses\n",
    "\n",
    "Now that we have modelled our system to be _contextually_ aware, it's time to test out it's awareness. I have turned on verbose reporting to give you an idea of the dialog flow, and how the context changes. You can see in certain situations, the bot prefers a particular intent even if it has a lower probability of prediction **if** the context is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the context and recreate the tags\n",
    "context = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches :  [('greeting', 0.88240492)]\n",
      "# :  {\n",
      "    \"tag\": \"greeting\",\n",
      "    \"patterns\": [\n",
      "        \"Hi\",\n",
      "        \"How are you\",\n",
      "        \"Is anyone there?\",\n",
      "        \"Hello\",\n",
      "        \"Good day\"\n",
      "    ],\n",
      "    \"responses\": [\n",
      "        \"Hello, thanks for visiting\",\n",
      "        \"Good to see you again\",\n",
      "        \"Hi there, how can I help?\"\n",
      "    ],\n",
      "    \"set_context\": [\n",
      "        \"hours\",\n",
      "        \"mopeds\"\n",
      "    ],\n",
      "    \"context_filter\": [\n",
      "        \"\",\n",
      "        \"greeting\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Good to see you again', [[], ['hours', 'mopeds']])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getResponse(\"Hi, how are you today?\", threshold=0.2, verbose=True), context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches :  [('opentoday', 0.99278331)]\n",
      "# :  {\n",
      "    \"tag\": \"opentoday\",\n",
      "    \"patterns\": [\n",
      "        \"Are you open today?\",\n",
      "        \"When do you open today?\",\n",
      "        \"What are your hours today?\"\n",
      "    ],\n",
      "    \"responses\": [\n",
      "        \"We're open every day from 9am-9pm\",\n",
      "        \"Our hours are 9am-9pm every day\"\n",
      "    ],\n",
      "    \"context_filter\": [\n",
      "        \"\",\n",
      "        \"opentoday\"\n",
      "    ],\n",
      "    \"set_context\": [\n",
      "        \"\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Our hours are 9am-9pm every day', [[], ['hours', 'mopeds'], ['']])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getResponse(\"Are you open today?\", threshold=0.2, verbose=True), context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], ['hours', 'mopeds'], ['']]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current context store, you can see how the context is set for the possible next question\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches :  [('rental', 0.87982565), ('thanks', 0.12016659)]\n",
      "# :  {\n",
      "    \"tag\": \"rental\",\n",
      "    \"patterns\": [\n",
      "        \"Can we rent a moped?\",\n",
      "        \"I'd like to rent a moped\",\n",
      "        \"How does this work?\"\n",
      "    ],\n",
      "    \"responses\": [\n",
      "        \"Are you looking to rent today or later this week?\"\n",
      "    ],\n",
      "    \"set_context\": [\n",
      "        \"rentalday\",\n",
      "        \"today\"\n",
      "    ],\n",
      "    \"context_filter\": [\n",
      "        \"rentquery\",\n",
      "        \"rental\"\n",
      "    ]\n",
      "}\n",
      "# :  {\n",
      "    \"tag\": \"thanks\",\n",
      "    \"patterns\": [\n",
      "        \"Thanks\",\n",
      "        \"Thank you\",\n",
      "        \"That's helpful\"\n",
      "    ],\n",
      "    \"responses\": [\n",
      "        \"Happy to help!\",\n",
      "        \"Any time!\",\n",
      "        \"My pleasure\"\n",
      "    ],\n",
      "    \"set_context\": [\n",
      "        \"\"\n",
      "    ],\n",
      "    \"context_filter\": [\n",
      "        \"\",\n",
      "        \"thanks\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Are you looking to rent today or later this week?',\n",
       " [[], ['hours', 'mopeds'], [''], ['rentalday', 'today']])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getResponse(\"Can I rent a moped today?\", threshold=0.1, verbose=True), context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches :  [('today', 0.5070141), ('opentoday', 0.47976768)]\n",
      "# :  {\n",
      "    \"tag\": \"today\",\n",
      "    \"patterns\": [\n",
      "        \"today\"\n",
      "    ],\n",
      "    \"responses\": [\n",
      "        \"For rentals today please call 1-800-MYMOPED\",\n",
      "        \"Same-day rentals please call 1-800-MYMOPED\"\n",
      "    ],\n",
      "    \"set_context\": [\n",
      "        \"rentquery\"\n",
      "    ],\n",
      "    \"context_filter\": [\n",
      "        \"rentalday\",\n",
      "        \"today\"\n",
      "    ]\n",
      "}\n",
      "# :  {\n",
      "    \"tag\": \"opentoday\",\n",
      "    \"patterns\": [\n",
      "        \"Are you open today?\",\n",
      "        \"When do you open today?\",\n",
      "        \"What are your hours today?\"\n",
      "    ],\n",
      "    \"responses\": [\n",
      "        \"We're open every day from 9am-9pm\",\n",
      "        \"Our hours are 9am-9pm every day\"\n",
      "    ],\n",
      "    \"context_filter\": [\n",
      "        \"\",\n",
      "        \"opentoday\"\n",
      "    ],\n",
      "    \"set_context\": [\n",
      "        \"\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Same-day rentals please call 1-800-MYMOPED',\n",
       " [[], ['hours', 'mopeds'], [''], ['rentalday', 'today'], ['rentquery']])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getResponse(\"are you in store today?\", threshold=0.2, verbose=True), context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Happy to help!',\n",
       " [[], ['hours', 'mopeds'], [''], ['rentalday', 'today'], ['rentquery'], ['']])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getResponse(\"thanks\", threshold=0.8), context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the bot is able to keep up contextually. Now obviously, due to the less number of training examples and ineffective context-filtering, this bot is still very primitive. Complex queries will stump it, and as a backup you should **always** have human intervention on stand-by. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
